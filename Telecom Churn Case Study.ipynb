{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5aad45",
   "metadata": {},
   "source": [
    "# Telecom Churn Case Study - Kaggle\n",
    "\n",
    "#### Created by: Akash Kriplani and Ankit Kr Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b417467",
   "metadata": {},
   "source": [
    "**Link:** https://www.kaggle.com/competitions/telecom-churn-case-study-hackathon-c56/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a73769",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business\n",
    "goal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn. In this project, you will analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn, and identify the main indicators of churn.\n",
    "\n",
    "In this competition, your goal is *to build a machine learning model that is able to predict churning customers based on the features provided for their usage.*\n",
    "\n",
    "**Customer behaviour during churn:**\n",
    "\n",
    "Customers usually do not decide to switch to another competitor instantly, but rather over a\n",
    "period of time (this is especially applicable to high-value customers). In churn prediction, we\n",
    "assume that there are three phases of customer lifecycle :\n",
    "\n",
    "1. <u>The ‘good’ phase:</u> In this phase, the customer is happy with the service and behaves as usual.\n",
    "\n",
    "2. <u>The ‘action’ phase:</u> The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. It is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\n",
    "\n",
    "3. <u>The ‘churn’ phase:</u> In this phase, the customer is said to have churned. In this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month (September) is the ‘churn’ phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d42f5a",
   "metadata": {},
   "source": [
    "## 1. Data Import and Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97992874",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/anandjha/Library/Python/3.9/lib/python/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B9202094-7D52-318C-99CF-7034B0E9F28D> /Users/anandjha/Library/Python/3.9/lib/python/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, AdaBoostClassifier\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RFE\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective, dask\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Booster,\n\u001b[1;32m     10\u001b[0m     DataIter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     build_info,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/tracker.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, make_jcargs\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:269\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m        return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:222\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[1;32m    221\u001b[0m         libname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) could not be loaded.\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124mLikely causes:\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124m  * OpenMP runtime is not installed\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;124m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[38;5;124mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m     _register_log_callback(lib)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(ver: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mXGBoostError\u001b[0m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/anandjha/Library/Python/3.9/lib/python/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B9202094-7D52-318C-99CF-7034B0E9F28D> /Users/anandjha/Library/Python/3.9/lib/python/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aea347-fb3f-444e-840f-e7a160441137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment settings\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa68aef-370f-4bf7-8dbc-4828bebc5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install libomp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab8dc0-8ab0-4d24-9133-6f59533b5271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "unseen = pd.read_csv(\"test.csv\")\n",
    "sample = pd.read_csv(\"sample.csv\")\n",
    "data_dict = pd.read_csv(\"data_dictionary.csv\")\n",
    "\n",
    "print(\"Dimension of the training dataset is\", train.shape)\n",
    "print(\"Dimension of the unseen dataset is\", unseen.shape)\n",
    "print(\"Dimension of the sample dataset is\", sample.shape)\n",
    "print(\"Dimension of the data dictionary is\", data_dict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d85c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the data dictionary\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762abe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the train dataset\n",
    "train.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba23b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics summary of the train dataset\n",
    "train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the copy of the original training set\n",
    "telecom_df = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b38c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the copy of the original test set\n",
    "telecom_test_df = unseen.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a6297",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47789a97",
   "metadata": {},
   "source": [
    "### 2.1. Checking the columns with single unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a507a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_single_value = []\n",
    "for col in train.columns:\n",
    "    if train[col].nunique() == 1:\n",
    "        cols_with_single_value.append(col)\n",
    "\n",
    "print(cols_with_single_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting circle id\n",
    "for col in cols_with_single_value:\n",
    "    print(col)\n",
    "    print(train[col].value_counts(dropna=False))\n",
    "    print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90684a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the dataset, we remove these columns from the training set\n",
    "train.drop(cols_with_single_value, axis=1, inplace=True)\n",
    "\n",
    "# Based on the dataset, we remove these columns from the test set\n",
    "unseen.drop(cols_with_single_value, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a684b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the id column since it is unique for every row from the training set\n",
    "train.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a list of column names based on their corresponding types\n",
    "\n",
    "date_cols = ['date_of_last_rech_6',\n",
    "            'date_of_last_rech_7',\n",
    "            'date_of_last_rech_8',\n",
    "            'date_of_last_rech_data_6',\n",
    "            'date_of_last_rech_data_7',\n",
    "            'date_of_last_rech_data_8',\n",
    "            ]\n",
    "\n",
    "cat_cols = ['night_pck_user_6',\n",
    "            'night_pck_user_7',\n",
    "            'night_pck_user_8',\n",
    "            'fb_user_6',\n",
    "            'fb_user_7',\n",
    "            'fb_user_8',\n",
    "           ]\n",
    "\n",
    "numerical_cols = [col for col in train.columns if col not in date_cols + cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping date columns as they are not useful for our analysis from the training set\n",
    "train.drop(columns=date_cols, axis=1, inplace=True)\n",
    "\n",
    "# Dropping date columns as they are not useful for our analysis from the test set\n",
    "unseen.drop(columns=date_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c683009",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for columns with more than 70% missing values in the training dataset\n",
    "mv_train_set = round(100*(train.isna().sum(axis=0)/train.shape[0]), 2)\n",
    "mv_train_set[mv_train_set > 70].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50dc956",
   "metadata": {},
   "source": [
    "There are numerous variables with consistent percentages of missing data, suggesting a pattern of meaningful absence. Our strategy involves grouping similar data for analysis:\n",
    "\n",
    "1. Grouping and treating missing values for 'recharge-related' data.\n",
    "2. Grouping and treating missing values for 'ARPU' data.\n",
    "3. Conducting missing value treatment on categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1adcc",
   "metadata": {},
   "source": [
    "#### Recharge Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1da690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with recharge information \n",
    "recharge_cols = train.columns[train.columns.str.contains('_rech_')]\n",
    "print(recharge_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9df67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values in recharge related columns with 0 in the training set\n",
    "train[recharge_cols] = train[recharge_cols].fillna(0)\n",
    "print(train[recharge_cols].isnull().sum()*100/train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6692e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values in recharge related columns with 0 in the test set\n",
    "unseen[recharge_cols] = unseen[recharge_cols].fillna(0)\n",
    "print(unseen[recharge_cols].isnull().sum()*100/unseen.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ba83a",
   "metadata": {},
   "source": [
    "#### ARPU Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with 'arpu' information \n",
    "arpu_cols = train.columns[train.columns.str.contains('arpu_')]\n",
    "print(arpu_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values in ARPU related columns with 0 in the training set\n",
    "train[arpu_cols] = train[arpu_cols].fillna(0)\n",
    "print(train[arpu_cols].isnull().sum()*100/train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e8679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values in ARPU related columns with 0 in the test set\n",
    "unseen[arpu_cols] = unseen[arpu_cols].fillna(0)\n",
    "print(unseen[arpu_cols].isnull().sum()*100/unseen.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d971c9",
   "metadata": {},
   "source": [
    "#### Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee851798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting categorical columns\n",
    "train[cat_cols].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b7c57",
   "metadata": {},
   "source": [
    "### 2.2. Imputing missing values in Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339da308",
   "metadata": {},
   "source": [
    "##### Imputing missing categorical values with a value of -1 facilitates the preservation and identification of missing data while ensuring compatibility with analysis algorithms and maintaining consistency in data encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a23ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values in categorical related columns with -1 in the training set.\n",
    "# We noticed that the columns listed in `cat_cols` are of the `float64` data type.\n",
    "# We'll need to convert them to the `category` data type.\n",
    "train[cat_cols] = train[cat_cols].fillna(-1).astype('category')\n",
    "print(train[cat_cols].isnull().sum()*100/train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab02b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the change in dtype on the train set\n",
    "train[cat_cols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN values in categorical related columns with -1 in the test set.\n",
    "# We noticed that the columns listed in `cat_cols` are of the `float64` data type.\n",
    "# We'll need to convert them to the `category` data type.\n",
    "unseen[cat_cols] = unseen[cat_cols].fillna(-1).astype('category')\n",
    "print(unseen[cat_cols].isnull().sum()*100/unseen.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the change in dtype on the test set\n",
    "unseen[cat_cols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f791457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the columns which have missing values present\n",
    "mv_train_set[(mv_train_set > 0) & (mv_train_set <= 70)].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec624313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list containing these columns to be processed later for imputation using the KNN technique.\n",
    "missing_values_list = mv_train_set[(mv_train_set > 0) & (mv_train_set <= 70)].index\n",
    "print(missing_values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb10a22",
   "metadata": {},
   "source": [
    "### 2.3. Deriving New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae26645",
   "metadata": {},
   "source": [
    "##### Filtering only High Value Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe7e0e",
   "metadata": {},
   "source": [
    "In this segment, we'll identify high-value customers aligning with our business objectives, focusing solely on prepaid customers experiencing usage-based churn. We'll evaluate the recharge amounts for June and July, selecting only the top 70% of customers as high-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3268b",
   "metadata": {},
   "source": [
    "We can create a new feature named ***Total Data Recharge Amount*** by multiplying the values of `total_rech_data` and `av_rech_amt_data`, which represents the amount recharged by the customer for data usage.\n",
    "\n",
    "- ***Total Data Recharge Amount = Total Data Recharge * Average Data Recharge Amount***\n",
    "\n",
    "Created another column for total recharge done ***Total Amount*** for the months 6 and 7.\n",
    "\n",
    "- ***Total Amount = Total Data Recharge Amount + Total Recharge Amount***\n",
    "\n",
    "Additionally, we can compute the ***Total Average Recharge Amount*** during the 'Good Phase', which includes months 6 and 7.\n",
    "\n",
    "- ***Total Average Amount = (Total Data Recharge Amount + Total Recharge Amount) / 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7fa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deriving new columns for Total Data Recharge Amount for the 'Good Phase' in the training set\n",
    "train['total_rech_data_amt_6'] = train['total_rech_data_6'] * train['av_rech_amt_data_6']\n",
    "train['total_rech_data_amt_7'] = train['total_rech_data_7'] * train['av_rech_amt_data_7']\n",
    "\n",
    "train['total_amt_6'] = train['total_rech_data_amt_6'] + train['total_rech_amt_6']\n",
    "train['total_amt_7'] = train['total_rech_data_amt_7'] + train['total_rech_amt_7']\n",
    "\n",
    "train['total_avg_amt_6_7'] = train[['total_amt_6', 'total_amt_7']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns as they are not so useful columns for high value customer or churn\n",
    "\n",
    "drop_data_cols = ['total_rech_data_amt_6', 'total_rech_data_amt_7', 'av_rech_amt_data_6', 'av_rech_amt_data_7',\n",
    "                  'av_rech_amt_data_8', 'total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8',]\n",
    "\n",
    "train.drop(drop_data_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cef1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deriving new columns for Total Data Recharge Amount for the 'Good Phase' in the test set\n",
    "unseen['total_rech_data_amt_6'] = unseen['total_rech_data_6'] * unseen['av_rech_amt_data_6']\n",
    "unseen['total_rech_data_amt_7'] = unseen['total_rech_data_7'] * unseen['av_rech_amt_data_7']\n",
    "\n",
    "unseen['total_amt_6'] = unseen['total_rech_data_amt_6'] + unseen['total_rech_amt_6']\n",
    "unseen['total_amt_7'] = unseen['total_rech_data_amt_7'] + unseen['total_rech_amt_7']\n",
    "\n",
    "unseen['total_avg_amt_6_7'] = unseen[['total_amt_6', 'total_amt_7']].mean(axis=1)\n",
    "\n",
    "# Dropping the drop_data_cols from the test set\n",
    "unseen.drop(drop_data_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bab43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any null value in the derived column in the training set\n",
    "train['total_avg_amt_6_7'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b77b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36276660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any null value in the derived column in the test set\n",
    "unseen['total_avg_amt_6_7'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bf015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unseen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding cutoff value for only top 70% of high value customers\n",
    "cutoff = train['total_avg_amt_6_7'].quantile(0.7)\n",
    "cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter high value customers in the new dataframe (high_val_train) where total recharge is more than 477.5\n",
    "high_val_train = train[(train['total_avg_amt_6_7'] > cutoff)].copy()\n",
    "\n",
    "print(high_val_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8296205",
   "metadata": {},
   "source": [
    "Henceforth, all our subsequent operations will be guided by the data set comprising these 20.9K high-value customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cad253",
   "metadata": {},
   "source": [
    "### 2.4. Analysing our target variable `churn_probability` and Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_val_train['churn_probability'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cb17eb",
   "metadata": {},
   "source": [
    "This variable needs to change to category dtype as it is a classification variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c96bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_val_train['churn_probability'] = high_val_train['churn_probability'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the new dtype\n",
    "high_val_train['churn_probability'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value counts of churn and not-churn\n",
    "print(high_val_train['churn_probability'].value_counts())\n",
    "\n",
    "print('-------------------------------------------------')\n",
    "\n",
    "# Calculate churn ratio\n",
    "print('Churn ratio:')\n",
    "print(high_val_train['churn_probability'].value_counts()*100/high_val_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd847d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize churn distribution\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.countplot(x = high_val_train['churn_probability'])\n",
    "plt.xlabel('Customer churn', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Customer churn distribution', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f1762",
   "metadata": {},
   "source": [
    "#### Based on the visualization, it's evident that 91.73% of customers are non-churn, while 8.26% of customers churn. This indicates a noticeable class imbalance, which we will address later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of high_val_train df and use the new df (churn_data) for pruning in the training set\n",
    "churn_data = high_val_train.copy()\n",
    "churn_data.reset_index(inplace=True)\n",
    "churn_data.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92dedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f28a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of columns with missing values in the training set\n",
    "mv = round(100*(churn_data.isnull().sum(axis=0)/churn_data.shape[0]), 2)\n",
    "s = mv.loc[mv>0]\n",
    "impute_cols_num = list(s.index)\n",
    "len(impute_cols_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to verify how many values are missing from the dataframe\n",
    "drop = high_val_train.dropna()\n",
    "print(churn_data.shape[0] - drop.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4112cbf",
   "metadata": {},
   "source": [
    "### 2.5. Impute missing values using KNN Imputer - Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645e691",
   "metadata": {},
   "source": [
    "We cannot discard these missing values because doing so would result in the loss of valuable information. Instead, we will employ imputation techniques such as KNNImputer.\n",
    "\n",
    "KNNImputer is a technique used to impute missing values in a dataset based on the values of its nearest neighbors. It works by identifying the k nearest neighbors of each data point with missing values, then averaging or taking a weighted average of the available values from those neighbors to fill in the missing value. This approach leverages the similarity between data points to estimate the missing values more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imputer instance\n",
    "imputer = KNNImputer(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate df having columns to be imputed\n",
    "df_num = churn_data[impute_cols_num]\n",
    "churn_df = churn_data.drop(impute_cols_num, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Impute using KNN imputer instance\n",
    "df_imp = imputer.fit_transform(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb92226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it to df post imputation\n",
    "df_imp = pd.DataFrame(df_imp, columns=impute_cols_num)\n",
    "df_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80589b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes together post imputation to create the final training set\n",
    "final_df = pd.concat([df_imp, churn_df], join='inner', axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73161941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no missing value is present in the training set now\n",
    "final_df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c33266c",
   "metadata": {},
   "source": [
    "### 2.6. Impute missing values using the KNN Imputer - Unseen Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1369f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate df having columns to be imputed\n",
    "df_num_unseen = unseen[impute_cols_num]\n",
    "churn_df_unseen = unseen.drop(impute_cols_num, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Impute using KNN imputer instance\n",
    "df_imp_unseen = imputer.transform(df_num_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it to df post imputation\n",
    "df_imp_unseen = pd.DataFrame(df_imp_unseen, columns=impute_cols_num)\n",
    "df_imp_unseen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02abf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes together post imputation\n",
    "final_unseen_df = pd.concat([df_imp_unseen, churn_df_unseen], join='inner', axis=1)\n",
    "final_unseen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a562c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_unseen_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no missing value is present in the test set now\n",
    "final_unseen_df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec79be9",
   "metadata": {},
   "source": [
    "#### There is no missing values in the dataset now. Let's perform Exploratory Data Analysis and Outlier Treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23e40f",
   "metadata": {},
   "source": [
    "### 3. Common Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cdfa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_graph(title, month, y_list, data):\n",
    "    \"\"\"\n",
    "    Plot bar graphs for given data.\n",
    "\n",
    "    Parameters:\n",
    "    - title (str): Title of the plot.\n",
    "    - month (list of str): List of month names.\n",
    "    - y_list (list of str): List of y-variable names corresponding to each month.\n",
    "    - data (DataFrame): DataFrame containing the data to be plotted.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    This function plots bar graphs for each month's data based on the provided y-variables.\n",
    "    It creates subplots for each month, with the corresponding y-variable plotted against 'churn_probability'.\n",
    "    The title of each subplot includes the specified title along with the month name.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, m in enumerate(month):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.title(f'{title} for {m}')\n",
    "        sns.barplot(x='churn_probability', y=y_list[i], data=data)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eded48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_handling(dataframe):\n",
    "    \"\"\"\n",
    "    Applies outlier handling transformations to the values in the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (DataFrame): The DataFrame containing the values to be transformed.\n",
    "\n",
    "    Returns:\n",
    "    - transformed_values (list): A list containing the transformed values based on the specified conditions.\n",
    "    \"\"\"\n",
    "\n",
    "    transformed_values = []\n",
    "    for value in dataframe:\n",
    "        if value <= 0:\n",
    "            transformed_values.append(float(0))\n",
    "        elif float(value) == float(1.0):\n",
    "            transformed_values.append(float(1.5))\n",
    "        else:\n",
    "            transformed_values.append(float(np.log1p(value)))\n",
    "\n",
    "    return transformed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692fe968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate various classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True labels.\n",
    "    y_pred (array-like): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('Confusion Matrix')\n",
    "    print()\n",
    "    print(cm)\n",
    "    \n",
    "    # True Positives, False Positives, True Negatives, False Negatives\n",
    "    TP = cm[1, 1]\n",
    "    FP = cm[0, 1]\n",
    "    TN = cm[0, 0]\n",
    "    FN = cm[1, 0]\n",
    "\n",
    "    # Accuracy Score\n",
    "    accuracy = round((TP + TN) / (TP + FP + TN + FN), 4)\n",
    "\n",
    "    # Sensitivity (Recall or True Positive Rate)\n",
    "    sensitivity = round(TP / (TP + FN), 4)\n",
    "\n",
    "    # Specificity\n",
    "    specificity = round(TN / (TN + FP), 4)\n",
    "\n",
    "    # Precision\n",
    "    precision = round(TP / (TP + FP), 4)\n",
    "\n",
    "    # False Positive Rate\n",
    "    false_positive_rate = round(FP / (FP + TN), 4)\n",
    "\n",
    "    # True Positive Rate\n",
    "    true_positive_rate = sensitivity\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'False Positive Rate', 'True Positive Rate'],\n",
    "        'Value': [accuracy, sensitivity, specificity, precision, false_positive_rate, true_positive_rate]\n",
    "    })\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc(actual, probs):\n",
    "    fpr, tpr, thresholds = roc_curve(actual, probs,drop_intermediate = False)\n",
    "    auc_score = roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC AUC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94badbfe",
   "metadata": {},
   "source": [
    "### 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Average Revenue per user\n",
    "arpu_cols = ['arpu_6', 'arpu_7', 'arpu_8']\n",
    "plot_bar_graph('Avg. Revenue Per User', ['June', 'July', 'August'], arpu_cols, final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119de5bb",
   "metadata": {},
   "source": [
    "##### The decline in the Average Revenue Per User (ARPU) in month of August suggests churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising Outgoing minutes of usage\n",
    "total_og_mou_cols = ['total_og_mou_6', 'total_og_mou_7', 'total_og_mou_8']\n",
    "plot_bar_graph('Total outgoing minutes', ['June', 'July', 'August'], total_og_mou_cols, final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8b702",
   "metadata": {},
   "source": [
    "##### The decline in the Total outgoing minutes in month of August suggests churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising incoming minutes of usage\n",
    "total_ic_mou_cols = ['total_ic_mou_6', 'total_ic_mou_7', 'total_ic_mou_8']\n",
    "plot_bar_graph('Total incoming minutes', ['June', 'July', 'August'], total_ic_mou_cols, final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a89981",
   "metadata": {},
   "source": [
    "##### The decline in the Total incoming minutes in month of August suggests churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ed9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing all types of calls within the operator's network.\n",
    "onnet_mou_cols = ['onnet_mou_6', 'onnet_mou_7', 'onnet_mou_8']\n",
    "plot_bar_graph('Calls within operator network', ['June', 'July', 'August'], onnet_mou_cols, final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52595304",
   "metadata": {},
   "source": [
    "##### The decline in the Calls within operator network in month of August suggests churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a62adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing all types of calls made outside the operator network.\n",
    "offnet_mou_cols = ['offnet_mou_6', 'offnet_mou_7', 'offnet_mou_8']\n",
    "plot_bar_graph('Calls outside operator network', ['June', 'July', 'August'], offnet_mou_cols, final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3015b",
   "metadata": {},
   "source": [
    "##### The decline in the Calls outside operator network in month of August suggests churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7004ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing different metrics by extracting them into columns of similar type\n",
    "amount_data = final_df.loc[:, final_df.columns.str.contains('amt')]\n",
    "data2g = final_df.loc[:, final_df.columns.str.contains('2g')]\n",
    "data3g = final_df.loc[:, final_df.columns.str.contains('3g')]\n",
    "ic_mou_data = final_df.loc[:, final_df.columns.str.contains('ic') & final_df.columns.str.contains('mou')]\n",
    "net_mou_data = final_df.loc[:, final_df.columns.str.contains('net_mou')]\n",
    "og_mou_data = final_df.loc[:, final_df.columns.str.contains('og') & final_df.columns.str.contains('mou')]\n",
    "rech_data = final_df.loc[:, final_df.columns.str.contains('rech')]\n",
    "total_data = final_df.loc[:, final_df.columns.str.contains('tot')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount data\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(amount_data.corr(), annot=True, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413faac",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "- A substantial correlation of 74% exists between the total recharge amount in month 7 and month 8.\n",
    "- Similarly, a notable correlation of 68% is observed between the maximum recharge amount in month 8 and the last day's recharge amount in the same month.\n",
    "- This suggests that customers who are unlikely to churn tend to recharge higher amounts in month 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad46cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2g\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.heatmap(data2g.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03165a61",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "- There is a strong correlation between recharge quantities in months 7 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3g\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(data3g.corr(),annot=True, cmap='Greens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f48584",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "- There is a strong correlation between recharge quantities in months 7 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ic_mou_data\n",
    "plt.figure(figsize=(25, 25))\n",
    "sns.heatmap(ic_mou_data.corr(), annot=True, cmap='Purples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b5689",
   "metadata": {},
   "source": [
    "***Observations:*** <br/><br/>\n",
    "The following variables or features exhibit a remarkably high correlation:\n",
    "\n",
    "1. total_ic_mou_6 and loc_ic_mou_6\n",
    "2. total_ic_mou_7 and loc_ic_mou_7\n",
    "3. total_ic_mou_8 and loc_ic_mou_8\n",
    "4. std_ic_mou_6 and std_ic_t2f_mou_6\n",
    "5. std_ic_mou_7 and std_ic_t2f_mou_7\n",
    "6. std_ic_mou_8 and std_ic_t2f_mou_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11870618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_mou_data\n",
    "sns.heatmap(net_mou_data.corr(), annot=True, cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ffae9",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "- There is no correlation between the usage minutes for on-net and off-net calls.\n",
    "- However, a significant correlation is observed between the on-net and off-net usage minutes in both months 7 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ec403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#og_mou_data\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(og_mou_data.corr(), annot=True, cmap='GnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb823aa",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "- There exists a very high correlation between the following variables/features:\n",
    "  1. isd_og_mou_7 and isd_og_mou_6\n",
    "  2. isd_og_mou_8 and isd_og_mou_6\n",
    "  3. isd_og_mou_8 and isd_og_mou_7\n",
    "  4. total_og_mou_6 and std_og_mou_6\n",
    "  5. total_og_mou_7 and std_og_mou_7\n",
    "  6. total_og_mou_8 and std_og_mou_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rech_data\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(rech_data.corr(), annot=True, cmap='ocean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53362f",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "- There is a notable correlation between the average recharge amount and the recharge amount for both months 6 and 7.\n",
    "- A significant correlation of 76% exists between the total recharge for month 7 and the recharge for month 8.\n",
    "- Any factor correlated with month 8 is likely correlated with churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2579429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_data\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(total_data.corr(), annot=True, cmap='YlGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f78a1",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "- There exists a remarkably high correlation between the incoming (82%) and outgoing minutes (77%) of usage in months 7 and 8.\n",
    "- Heavy usage suggests that customers are less likely to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bba033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting shape of the training dataset\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ec6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the shape of the test set\n",
    "final_unseen_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unseen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unseen_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be034ea1",
   "metadata": {},
   "source": [
    "### 5. Dummy Variable Creation (One Hot Encoding) - Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummies for categorical columns\n",
    "dummy_df = pd.get_dummies(final_df[['night_pck_user_8', 'fb_user_8', 'fb_user_7', 'fb_user_6', 'night_pck_user_7', 'night_pck_user_6']], drop_first=True, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([final_df, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the category cols\n",
    "final_df.drop(['night_pck_user_8', 'fb_user_8', 'fb_user_7', 'fb_user_6', 'night_pck_user_7', 'night_pck_user_6'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b0c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88015721",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6ae14",
   "metadata": {},
   "source": [
    "### 6. Dummy Variable Creation (One Hot Encoding) - Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummies for categorical columns\n",
    "dummy_unseen_df = pd.get_dummies(final_unseen_df[['night_pck_user_8', 'fb_user_8', 'fb_user_7', 'fb_user_6', 'night_pck_user_7', 'night_pck_user_6']], drop_first=True, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd165bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unseen_df = pd.concat([final_unseen_df, dummy_unseen_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the category cols\n",
    "final_unseen_df.drop(['night_pck_user_8', 'fb_user_8', 'fb_user_7', 'fb_user_6', 'night_pck_user_7', 'night_pck_user_6'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a844860",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unseen_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unseen_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf385cd7",
   "metadata": {},
   "source": [
    "### 7. Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380473c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\n",
    "final_df.describe(percentiles=[.25, .50, .75, .90, .95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = list(final_df.select_dtypes(exclude='category'))\n",
    "numerical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da84e41",
   "metadata": {},
   "source": [
    "##### The `outlier_handling` method treats outliers and adjusts skewness in the dataset to improve data robustness and analysis accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers of numerical columns in the training set\n",
    "final_df[numerical_cols] = final_df[numerical_cols].apply(outlier_handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the numerical columns again in the training set\n",
    "final_df.describe(percentiles=[.25, .50, .75, .90, .95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8042e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers of numerical columns in the test set\n",
    "final_unseen_df[numerical_cols] = final_unseen_df[numerical_cols].apply(outlier_handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ed446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the numerical columns again in the test set\n",
    "final_unseen_df.describe(percentiles=[.25, .50, .75, .90, .95, .99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a49d8d",
   "metadata": {},
   "source": [
    "### 8. Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfafca0",
   "metadata": {},
   "source": [
    "#### Train-Test Split\n",
    "\n",
    "Given the availability of the `test.csv` file for testing purposes, this train-test split will exclusively operate on the `train.csv` dataset. It involves allocating 80% of the data for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a21491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the dataset into the feature variable 'X' and the response variable 'y'.\n",
    "X = final_df.drop(['churn_probability'], axis=1)\n",
    "y = final_df['churn_probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a516eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6af705",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d156f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split - 80% training data, 20% validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape after split\n",
    "print('X_train Shape:', X_train.shape)\n",
    "print('y_train Shape:', y_train.shape)\n",
    "print('X_val Shape:', X_val.shape)\n",
    "print('y_val Shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599951d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab976aba",
   "metadata": {},
   "source": [
    "### 8.1. Handling Data Imbalance using Synthetic Minority Over-sampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=2)\n",
    "X_train_smote, y_train_smote = smt.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution on our target variable 'y_train' before and after SMOTE\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Distribution of churn (imbalanced)\")\n",
    "sns.countplot(x=y_train, hue=y_train)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Distribution of churn (balanced)\")\n",
    "sns.countplot(x=y_train_smote, hue=y_train_smote)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fdeba",
   "metadata": {},
   "source": [
    "##### Now the data imbalance issue is sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5507e21",
   "metadata": {},
   "source": [
    "### 8.2. Feature Scaling using SandardScaler - Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SandardScaler to scale values\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform training set\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_smote), columns = X_train_smote.columns, index = X_train_smote.index)\n",
    "\n",
    "# Only Transform validation set\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns = X_val.columns, index = X_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de67ed",
   "metadata": {},
   "source": [
    "### 8.3. Feature Scaling using SandardScaler - Unseen Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unseen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac571b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unseen_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d61ee",
   "metadata": {},
   "source": [
    "### Model 1. Logistic Regression with Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Select top 20 features using RFE\n",
    "logreg = LogisticRegression(max_iter=1000, tol=0.001, solver='sag')\n",
    "rfe = RFE(logreg, n_features_to_select=20)\n",
    "rfe = rfe.fit(X_train_scaled, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854cb15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train_scaled.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883747c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected features using RFE\n",
    "selected_features = X_train_scaled.columns[rfe.support_]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_final = X_train_scaled[selected_features]\n",
    "logreg.fit(X_train_scaled_final, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_smote_pred = logreg.predict(X_train_scaled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics on the training set\n",
    "calculate_classification_metrics(y_train_smote, y_train_smote_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only the selected feature from the final model\n",
    "X_val_scaled_final = X_val_scaled[selected_features]\n",
    "X_val_scaled_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bccea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values from the linear model\n",
    "y_val_pred = logreg.predict(X_val_scaled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccfd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_true=y_val, y_pred=y_val_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a0042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics_df = calculate_classification_metrics(y_true=y_val, y_pred=y_val_pred)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9710af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels for the confusion matrix\n",
    "labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "\n",
    "# Reshape the confusion matrix into a 2x2 array\n",
    "cm_array = np.array(confusion).reshape(2,2)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(cm_array, annot=True, cmap='Oranges', fmt='g', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02400743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = draw_roc(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2af650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUC Score\n",
    "\"{:2.2f}\".format(roc_auc_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669240df",
   "metadata": {},
   "source": [
    "### Model 2. Logistic Regression with PCA and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057df92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = pca.fit_transform(X_train)\n",
    "pca_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=pca_components[:,0], y=pca_components[:,1], hue=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848df734",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=pca_components[:,1], y=pca_components[:,2], hue=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39ac85",
   "metadata": {},
   "source": [
    "##### As we can see, there is a clear class imbalance and we have addressed this below using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19463e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = high_val_train.copy().drop(['churn_probability'], axis=1)\n",
    "y = high_val_train.copy()['churn_probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95779a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfd180",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=20)\n",
    "smote = SMOTE(random_state=2)\n",
    "lr = LogisticRegression(max_iter=1000, tol=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86731b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    imputer,\n",
    "    smote,\n",
    "    scaler,\n",
    "    pca,\n",
    "    lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logisticregression__solver': ['liblinear', 'saga'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a157af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV\n",
    "lr_grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df128eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using GridSearchCV\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and estimator\n",
    "lr_best_params = lr_grid_search.best_params_\n",
    "lr_best_estimator = lr_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626444e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values on the training set\n",
    "y_train_pred = lr_grid_search.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeec406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values on the validation set\n",
    "y_val_pred = lr_grid_search.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics of the training set\n",
    "calculate_classification_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics of the validation set\n",
    "calculate_classification_metrics(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = draw_roc(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a11c9",
   "metadata": {},
   "source": [
    "### Model 3. Decision Tree with PCA and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ac940",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1541069",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=25)\n",
    "smote = SMOTE(random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = make_pipeline(\n",
    "    imputer,\n",
    "    smote,\n",
    "    scaler,\n",
    "    pca,\n",
    "    dt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "params = {\n",
    "    'decisiontreeclassifier__max_depth': [2, 3, 5, 10, 20],\n",
    "    'decisiontreeclassifier__min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ba951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=params, \n",
    "                           cv=5, n_jobs=-1, verbose=1, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Predict on train set using the best model\n",
    "y_train_pred = grid_search.predict(X_train)\n",
    "\n",
    "# Predict on validation set using the best model\n",
    "y_val_pred = grid_search.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the train set\n",
    "calculate_classification_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the validation set\n",
    "calculate_classification_metrics(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08774a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = draw_roc(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0750e",
   "metadata": {},
   "source": [
    "### Model 4. Random Forest Classifier with PCA and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1765001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = make_pipeline(\n",
    "    imputer,\n",
    "    smote,\n",
    "    scaler,\n",
    "    pca,\n",
    "    rf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb8b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters grid for Random Forest\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [100],  \n",
    "    'randomforestclassifier__max_depth': [5, 10],       \n",
    "    'randomforestclassifier__min_samples_split': [2, 5, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a299dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba137d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on train set using the best model\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "# Predict on train set using the validation model\n",
    "y_test_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the train set\n",
    "calculate_classification_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the validation set\n",
    "calculate_classification_metrics(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d340ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = draw_roc(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6dce1d",
   "metadata": {},
   "source": [
    "### Model 5. AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = make_pipeline(\n",
    "    imputer,\n",
    "    smote,\n",
    "    scaler,\n",
    "    pca,\n",
    "    AdaBoostClassifier(n_estimators=200, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6be45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "adaboost_model = pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on train set using the best model\n",
    "y_train_pred = adaboost_model.predict(X_train)\n",
    "\n",
    "# Predict on train set using the validation model\n",
    "y_test_pred = adaboost_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a69215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the train set\n",
    "calculate_classification_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff87e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the validation set\n",
    "calculate_classification_metrics(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f990dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = draw_roc(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00746aea",
   "metadata": {},
   "source": [
    "### Model 6. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae96c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe = make_pipeline(\n",
    "    imputer,\n",
    "    smote,\n",
    "    scaler,\n",
    "    pca,\n",
    "    XGBClassifier(random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec89f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'xgbclassifier__n_estimators': [100, 200, 300],  \n",
    "    'xgbclassifier__max_depth': [3, 5, 7],\n",
    "    'xgbclassifier__learning_rate': [0.01, 0.1, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bdd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on train set using the best model\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "# Predict on train set using the validation model\n",
    "y_test_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the train set\n",
    "calculate_classification_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f93639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the validation set\n",
    "calculate_classification_metrics(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16945cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = draw_roc(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8fd5d0",
   "metadata": {},
   "source": [
    "### 9. Create `submission.csv` file for final submission on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61851760",
   "metadata": {},
   "source": [
    "For our Kaggle submission, we've opted for logistic regression with Recursive Feature Elimination (RFE) due to the platform's evaluation metric being accuracy. This model consistently achieves approximately 85% accuracy on both the training and test sets. Notably, it also demonstrates a commendable accuracy of 93.8% on previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ccc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = final_unseen_df[selected_features]\n",
    "submission_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unseen_df['churn_probability'] = logreg.predict(submission_data)\n",
    "output = final_unseen_df[['churn_probability']]\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('submission.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e8823",
   "metadata": {},
   "source": [
    "### 10. Business Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c46d90",
   "metadata": {},
   "source": [
    "Based on the analysis of our logistic regression model with RFE, here are some business ideas to improve churn rate:\n",
    "\n",
    "1. **Roaming Offers**: Provide personalized roaming packages to frequent roamers.\n",
    "2. **Local Call Promotions**: Offer competitive rates and bonuses for local calls.\n",
    "3. **Data Recharge Strategies**: Promote data packs with targeted marketing campaigns.\n",
    "4. **High-Value Recharge Incentives**: Offer discounts for high-value recharges to retain customers.\n",
    "5. **Service Engagement Initiatives**: Enhance engagement through loyalty programs and personalized offers.\n",
    "6. **Retention Campaigns**: Target customers with low recharge activity with special offers.\n",
    "7. **Non-Data User Promotions**: Encourage non-data users to try data services with bundle offers.\n",
    "8. **Night Pack Revival**: Revive night pack usage through attractive offers and incentives.\n",
    "\n",
    "Implementing these strategies can effectively reduce churn and improve customer retention in your telecom business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d1937",
   "metadata": {},
   "source": [
    "### 11. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72415f30",
   "metadata": {},
   "source": [
    "After experimenting with various models, including Logistic Regression with Recursive Feature Elimination (RFE), Logistic Regression with hyperparameter tuning, and PCA, as well as Decision Tree, Random Forest, Adaboost, and XGBoost classifiers with hyperparameter tuning and PCA, it's evident that only Logistic Regression with PCA consistently demonstrates the highest sensitivity in both the train and validation sets. Consequently, this model should be considered as the final choice. Other models, although showing promising accuracy in the training phase, perform poorly on the test set, suggesting overfitting.\n",
    "\n",
    "In the context of telecom churn, where minimizing churn rate is crucial, sensitivity emerges as the most pertinent metric. Hence, based on this criterion, the Logistic Regression model with PCA stands out as the most suitable choice among all alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70403be",
   "metadata": {},
   "source": [
    "### 12. Important features - Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9e332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coefficients = lr_best_estimator.named_steps['logisticregression'].coef_\n",
    "feature_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8eb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, component in enumerate(feature_coefficients[0]):\n",
    "    print(f\"PC{i+1}: {component}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
